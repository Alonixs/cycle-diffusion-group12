{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 12 Mini-project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a conda virtual environment is existing and selected as the kernel to run this notebook. It can be created and activated using:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "source activate generative_prompt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate into the source code\n",
    "\n",
    "%cd ../src/cycle-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install torch and torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install taming-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CompVis/taming-transformers.git\n",
    "%cd taming-transformers/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. login to wandDB, adapt the configuration function,`setup_wandb()`, in `main.py`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation data\n",
    "4 Prepare the AFHQ validation set for unpaired image-to-image translation (also for some images used by zero-shot image-to-image translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd cycle-diffusion/\n",
    "!git clone https://github.com/clovaai/stargan-v2.git\n",
    "%cd stargan-v2/\n",
    "!bash download.sh afhq-v2-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained diffusion models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Stable Diffusion\n",
    "\n",
    "#TODO Manually install pre-trained checkpoints: https://huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ckpts/\n",
    "!mkdir stable_diffusion\n",
    "%cd stable_diffusion/\n",
    "!wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Latent Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/9lpdgs83l7tjk6c/ldm_models.zip\n",
    "!unzip ldm_models.zip\n",
    "!rm ldm_models.zip \n",
    "\n",
    "%cd ldm_models/\n",
    "!mkdir text2img-large\n",
    "%cd text2img-large/\n",
    "!wget https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n",
    "!wget https://www.dropbox.com/s/7pdttimz78ll0km/txt2img-1p4B-eval.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. DDPM (AFHQ-Dog and FFHQ are from ILVR; CelebAHQ is from SDEdit; AFHQ-Cat and -Wild are trained by ourselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ddpm\n",
    "%cd ddpm/\n",
    "# !wget https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt #TODO did not work\n",
    "!wget https://www.dropbox.com/s/g4h8sv07i3hj83d/ffhq_10m.pt\n",
    "!wget https://www.dropbox.com/s/u74w8vaw1f8lc4k/afhq_dog_4m.pt\n",
    "!wget https://www.dropbox.com/s/8i5aznjwdl3b5iq/cat_ema_0.9999_050000.pt\n",
    "!wget https://www.dropbox.com/s/tplximipy8zxaub/wild_ema_0.9999_050000.pt\n",
    "!wget https://www.dropbox.com/s/vqm6bxj0zslrjxv/configs.zip\n",
    "!unzip configs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../../.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd src/cycle-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import PIL\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display the first image in the first subplot\n",
    "img_cat = imread('stargan-test/data/test/cat/flickr_cat_000008.png')\n",
    "axes[0].imshow(img_cat)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Display the second image in the second subplot\n",
    "img_dog = imread('stargan-test/data/test/dog/flickr_dog_000054.png')\n",
    "axes[1].imshow(img_dog)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_filter(img, ftype, radius=32):\n",
    "    # do dft saving as complex output\n",
    "    dft = np.fft.fft2(img, axes=(0,1))\n",
    "\n",
    "    # apply shift of origin to center of image\n",
    "    dft_shift = np.fft.fftshift(dft)\n",
    "\n",
    "    # generate spectrum from magnitude image (for viewing only)\n",
    "    mag = np.abs(dft_shift)\n",
    "    spec = np.log(mag) / 20\n",
    "\n",
    "    # create circle mask\n",
    "    # radius = 32\n",
    "    mask = np.zeros_like(img)\n",
    "    cy = mask.shape[0] // 2\n",
    "    cx = mask.shape[1] // 2\n",
    "    cv2.circle(mask, (cx,cy), radius, (255,255,255), -1)[0]\n",
    "    if ftype == 'high':\n",
    "        mask = 255 - mask\n",
    "\n",
    "    # blur the mask\n",
    "    mask = cv2.GaussianBlur(mask, (19,19), 0)\n",
    "\n",
    "    # apply mask to dft_shift\n",
    "    dft_shift_masked = np.multiply(dft_shift,mask) / 255\n",
    "\n",
    "\n",
    "    # shift origin from center to upper left corner\n",
    "    back_ishift = np.fft.ifftshift(dft_shift)\n",
    "    back_ishift_masked = np.fft.ifftshift(dft_shift_masked)\n",
    "\n",
    "\n",
    "    # do idft saving as complex output\n",
    "    img_back = np.fft.ifft2(back_ishift, axes=(0,1))\n",
    "    img_filtered = np.fft.ifft2(back_ishift_masked, axes=(0,1))\n",
    "\n",
    "    # combine complex real and imaginary components to form (the magnitude for) the original image again\n",
    "    img_back = np.abs(img_back).clip(0,255).astype(np.uint8)\n",
    "    img_filtered = np.abs(img_filtered).clip(0,255).astype(np.uint8)\n",
    "\n",
    "    if ftype == 'high':\n",
    "        img_filtered = np.abs(3*img_filtered).clip(0,255).astype(np.uint8)\n",
    "    else:\n",
    "        img_filtered = np.abs(img_filtered).clip(0,255).astype(np.uint8)\n",
    "\n",
    "    return img_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cat_low = pass_filter(img_cat, 'low')\n",
    "img_cat_high = pass_filter(img_cat, 'high')\n",
    "img_dog_low = pass_filter(img_dog, 'low')\n",
    "img_dog_high = pass_filter(img_dog, 'high')\n",
    "\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "plt.imshow(img_cat_low)\n",
    "plt.savefig(\"stargan-test/data/test/cat/img_cat_low.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "plt.imshow(img_cat_high)\n",
    "plt.savefig(\"stargan-test/data/test/cat/img_cat_high.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "plt.imshow(img_dog_low)\n",
    "plt.savefig(\"stargan-test/data/test/dog/img_dog_low.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "plt.imshow(img_dog_high)\n",
    "plt.savefig(\"stargan-test/data/test/dog/img_dog_high.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "print(\"Saved all images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axes[0][0].imshow(img_cat_low)\n",
    "axes[0][0].axis('off')\n",
    "\n",
    "axes[1][0].imshow(img_cat_high)\n",
    "axes[1][0].axis('off')\n",
    "\n",
    "\n",
    "axes[0][1].imshow(img_dog_low)\n",
    "axes[0][1].axis('off')\n",
    "\n",
    "axes[1][1].imshow(img_dog_high)\n",
    "axes[1][1].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
