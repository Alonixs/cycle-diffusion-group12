#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=exp_1_2
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=2:00:00
#SBATCH --mem=32000M
#SBATCH --output=output_experiment_1_2_first_2_cats%A.out

module purge
module load 2021
module load Anaconda3/2021.05

source activate generative_prompt


export CUDA_VISIBLE_DEVICES=0
export RUN_NAME=experiment_1_2
export SEED=42
export LOCAL_RANK=0
export OUTPUT_DIR="${RUN_NAME}_${SEED}"

python -m torch.distributed.launch --nproc_per_node 1 --master_port 1446 main.py --seed $SEED --cfg experiments/$RUN_NAME.cfg --run_name $RUN_NAME$SEED --logging_strategy steps --logging_first_step true --logging_steps 4 --evaluation_strategy steps --eval_steps 50 --metric_for_best_model CLIPEnergy --greater_is_better false --save_strategy steps --save_steps 50 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs 0 --adafactor false --learning_rate 1e-3 --do_eval --output_dir output/$OUTPUT_DIR --overwrite_output_dir --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --eval_accumulation_steps 4 --ddp_find_unused_parameters true --verbose true --disable_wandb True --save_images True --custom_z_name rotated_images

conda deactivate
